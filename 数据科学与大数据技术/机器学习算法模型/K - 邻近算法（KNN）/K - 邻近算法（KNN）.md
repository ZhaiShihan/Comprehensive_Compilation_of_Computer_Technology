#### 算法简介

###### · K - 邻近算法介绍：
· K - 邻近算法（K - Nearest Neighbor，KNN）是一种基于实例的学习方法，不需要提前训练模型，而是直接使用所有的训练数据进行预测；当有一个新数据点需要分类或预测时，KNN 会看看这个新数据点周围最接近的 K 个数据点，然后根据这些邻居的数据来决定新数据点的类别或预测值


#### 理论基础：

###### · 基本步骤：
1. 选择 K 值：K 代表要找多少个最近的邻居，K 可以是 1、3、5 等；K 的值会影响结果，通常需要通过试验来找到一个合适的值
2. 计算距离：对于新数据点，计算它到所有训练数据点的距离，常用的距离计算方法是欧几里得距离（两点之间的直线距离）
3. 找出最近的 K 个邻居：根据计算出来的距离，选出距离最近的 K 个邻居
4. 做出预测：
	1. 分类任务：看这 K 个邻居中，哪个类别出现的次数最多，就把新数据点归为哪个类别
	2. 回归任务：取这 K 个邻居的数值的平均值，作为新数据点的预测值

###### · 初浅示例：
· 有一个水果商，有一批水果需要分类成苹果和橙子，它们的大小和颜色数据已知，如这样：
1. 水果 1：大小中等，颜色红，已知是苹果
2. 水果 2：大小大，颜色橙，已知是橘子
3. 水果 3：大小小，颜色红，已知是苹果
……
· 现在来了一颗新的水果，它的大小是中等，颜色是红色，水果商想知道它是苹果还是橙子

· 算法原理：
1. 选择 K=3，表示要看最接近的 3 个水果
2. 计算这颗新水果和所有已知水果的距离
3. 找出距离最近的 3 个已知水果
4. 这 3 个水果中，如果有 2 个是苹果，1 个是橙子，那么就可以推测新水果是苹果，因为多数是苹果

###### · 数学原理和公式推理：
1. 距离度量：KNN 的核心思想是根据距离来判断数据点的类别或回归值，常用的距离度量是欧几里得距离：给定两个点 $x=(x_1,x_2,\ldots,x_n)$ 和 $y_n=(y_1,y_2,\ldots,y_n)$，其欧几里得距离可以表示为：$$d(x,y)=\sqrt(\sum_{i=1}^n(x_i-y_i)^2)$$其他常用的距离度量包括曼哈顿距离和闵可夫斯基距离等

2. 分类任务：对于分类任务，KNN 会根据 K 个最近邻的类别进行多数表决，假设有一个待分类点 $y$，其 $K$ 个最邻近点的类别分别为 $y_1,y_2,\ldots,y_K$，则 $y$ 的类别 $\hat{y}$ 由以下公式确定：$$\hat{y}=mode(y_1,y_2,\ldots,y_K)$$其中 $mode$ 表示众数，即出现次数最多的类别

3. 回归任务：KNN 会根据 K 个最邻近点的数值进行平均，假设有一个待预测点 $y$，其 $K$ 个最邻近点的数值分别为 $y_1,y_2,\ldots,y_K$，则 $y$ 的预测值 $\hat{y}$ 由以下公式确定：$$\hat{y}=\frac{1}{K}\sum_{i=1}^Ky_i$$

###### · 算法流程：
1. 准备数据：准备训练数据集
2. 选择参数 K：选择一个合适的 K 值，K 通常是一个小的正整数，可以通过交叉验证等方法来选择最佳的 K 值
3. 计算距离：对于待预测的点，计算它到训练集中所有数据点的距离；假设训练集中有 N 个数据点，距离计算公式：$$d(x,x_i)=\sqrt{\sum_{j=1}^n(x_j-x_{ij})^2}$$
4. 选择 K 个最近邻：根据计算出的距离对所有训练数据点进行排序，选择距离最近的 K 个数据点，这些点的索引可以表示为 $y_1,y_2,\ldots,y_K$
5. 做出预测：对于分类任务，统计这 $K$ 个最近邻点的类别频率，选择出现频率最高的类别作为预测结果：$$\hat{y}=mode(y_{i_1},y_{i_2},\ldots,y_{i_K})$$
6. 返回结果：将预测结果 $\hat{y}$ 返回给用户


#### 详细示例：

###### · 详细推理示例：
· 假设有如下二维平面上的训练数据点及其类别：

| 数据点 | 坐标（x，y） | 类别  |
| :-- | :------ | :-- |
| 点 A | （1，2）   | 0   |
| 点 B | （2，3）   | 0   |
| 点 C | （3，3）   | 1   |
| 点 D | （6，5）   | 1   |
现在有一个待分类的新数据点 $x=(3,2)$，选择 $K=3$，步骤如下：

1. 计算距离：
	1. 点 A 到 x 的距离：$d(A,x)=\sqrt{(1-3)^2+(2-2)^2}=\sqrt{4}=2$
	2. 点 B 到 x 的距离：$d(B,x)=\sqrt{(2-3)^2+(3-2)^2}=\sqrt{1+1}=\sqrt{2}\approx 1.41$
	3. 点 C 到 x 的距离：$d(C,x)=\sqrt{(13-3)^2+(3-2)^2}=\sqrt{1}=1$
	4. 点 D 到 x 的距离：$d(C,x)=\sqrt{(6-3)^2+(5-2)^2}=\sqrt{9+9}=\sqrt{18}\approx 4.24$
2. 选择 $K$ 个最邻近：最近的三个邻居是点 C、点 B 和点 A
3. 做出预测：最近邻的类别分别是：1、0、0，多数类别是 0，因此预测结果是 0

###### · 鸢尾花数据集的 KNN 分类：

· 使用 KNN 算法对著名的鸢尾花（Iris）数据集进行分类，并进行算法优化
· 鸢尾花数据集包含 150 个样本，每个样本有四个特征：花萼长度、花萼宽度、花瓣长度、花瓣宽度
· 目标是根据这些特征将鸢尾花分为三类：Setoga、Versicolour、Virginica

1. 数据准备和可视化：
```Python
import numpy as np  
import pandas as pd  
from sklearn import datasets  
from sklearn.model_selection import train_test_split  
from sklearn.preprocessing import StandardScaler  
from sklearn.neighbors import KNeighborsClassifier  
from sklearn.metrics import accuracy_score, classification_report  
import matplotlib.pyplot as plt  
import seaborn as sns

# 加载鸢尾花数据集
iris = datasets.load_iris()
X = iris.data
y = iris.target
df = pd.DataFrame(data=np.c_[X, y], columns=iris.feature_names + ['target'])

# 数据可视化
sns.pairplot(df, hue='target', markers=["o", "s", "D"])
plt.show()
```
![](KNN%20图1.png)
（图一：用 K - 邻近算法绘制鸢尾花数据集的特征）

2. 数据预处理：
```Python
# 数据集分割  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  
  
# 特征标准化  
scaler = StandardScaler()  
X_train = scaler.fit_transform(X_train)  
X_test = scaler.transform(X_test)
```

3. 分割数据集：
```Python

```





#### 模型分析

###### · K - 邻近算法的优点：
1. 简单易懂：KNN 是一个非常直观且简单易实现的算法，适合初学者理解和使用
2. 无参数模型：KNN 不需要训练过程，只需要保存训练数据即可以预测
3. 适用于多分类问题：KNN 可以很好地处理多分类问题，如鸢尾花数据集中的三个分类
4. 灵活性强：通过调整 K 值可以控制模型的复杂度，容易进行调优

###### · K - 邻近算法的缺点：
1. 计算复杂度高：KNN 需要计算待预测点与所有训练数据点的距离，数据集较大时加算量非常大
2. 内存消耗大：KNN 需要存储所有训练数据，内存消耗较大，尤其是对于大规模数据集
3. 对特征尺度敏感：KNN 对特征的尺度非常敏感，因此需要进行数据标准化处理
4. 对特征尺度敏感：KNN 对特征的尺度非常敏感，因此需要进行数据标准化处理
5. 噪声敏感：KNN 对噪声数据敏感，容易受离群点影响

###### · 与相似算法的对比：
1. 计算复杂度高：KNN 需要计算待预测点与所有训练数据点的距离，数据集较大时计算量非常大
2. 内存消耗大：KNN 需要存储所有训练数据，内对存消耗较大，尤其是对于大规模数据集
3. 对特征尺度敏感：KNN 对特征的尺度非常敏感，因此需要进行数据标准化处理
4. 噪声敏感：KNN 对噪声数据敏感，容易受离群点影响

###### · KNN 与支持向量机（SVM）的比较：
1. 支持向量机（SVM）：通过找到最佳超平面来进行分类，适用于二分类和多分类
	1. 优点：在高维空间中表现良好，对噪声不敏感，可以通过核函数处理非线性问题
	2. 缺点：计算复杂度高，对参数和核函数的选择敏感
2. KNN：基于邻近样本的标签进行分类或回归
	1. 优点：简单直观，适用于多分类问题
	2. 缺点：对数据尺度敏感，计算复杂度高

###### · 对于 KNN 的选择：
1. 小数据集：数据集较小时，KNN 的计算复杂度不高，可以快速计算
2. 数据分布较简单：数据分布较简单且没有明显的噪声时，KNN 可以很好地进行分类
3. 特征数量较少：特征数量较少的情况下，KNN 的计算效率较高且性能较好
4. 初步模型测试：可以作为基准模型，快速评估数据集的基本情况

###### · 考虑其他算法的情况：
1. 大数据集：数据集较大时，KNN 的计算复杂度和内存消耗过高，应考虑其他更高效的算法，如随机森林、SVM 等
2. 高维数据：特征数量较多时，KNN 的计算效率下降，应考虑降维技术或其他算法，如 PCA 结合 SVM
3. 数据尺度不一致：如果数据特征的尺度差异较大且难以标准化，可以选择对尺度不敏感的算法，如决策树、随机森林
4. 需要模型解释性：如果需要对模型的决策过程进行解释，可以选择易于解释的算法，如决策树



~~~
内容整理自：
1. cos大壮-“深夜努力写Python”：《讲透一个强大算法模型，KNN！！》. 2024.5.31
~~~